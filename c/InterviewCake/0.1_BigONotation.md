# Table of Contents

1.  [Summary](#org8d70bb0)
2.  [Introduction](#orgb5db641)
    1.  [Big O notation describes how long an algorithm takes to run.](#org16f07d0)
    2.  [We use to express the following 3 things:](#org20ddb9f)
        1.  [**How quickly the runtime grows**:](#org92c41cb)
        2.  [**Relative to the input**:](#org2ca7520)
        3.  [**As the input gets arbitrarily large**: —Our algorithm may have steps that seem expensive when nn is small but are eclipsed eventually by other steps as nn gets huge. For big O analysis, we care most about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as nn gets very large. (If you know what an asymptote is, you might see why &ldquo;big O analysis&rdquo; is sometimes called &ldquo;asymptotic analysis.&rdquo;)](#org204db25)

<a id="org8d70bb0"></a>

# Summary

<a id="orgb5db641"></a>

# Introduction

<a id="org16f07d0"></a>

## Big O notation describes how long an algorithm takes to run.

<a id="org20ddb9f"></a>

## We use to express the following 3 things:

<a id="org92c41cb"></a>

### **How quickly the runtime grows**:

1.  It&rsquo;s hard to pin down the exact runtime of an algorithm, since it depends on the speed of the processor, other processes, etc.

2.  So instead of talking about the runtime directly, we use big O notation to talk about how quickly the runtime grows.

<a id="org2ca7520"></a>

### **Relative to the input**:

1.  If we were measuring our runtime directly, we could express our speed in seconds.

2.  Since we&rsquo;re measuring how quickly our runtime grows, we need to express our speed in terms of&#x2026;something else.

3.  With Big O notation, we use the size of the input, which we call \(n\) So we can say things like the runtime grows &ldquo;on the order of the size of the input&rdquo; (O(n)O(n)) or &ldquo;on the order of the square of the size of the input&rdquo; (O(n<sup>2</sup>)O(n<sup>2</sup>)).

<a id="org204db25"></a>

### **As the input gets arbitrarily large**: —Our algorithm may have steps that seem expensive when nn is small but are eclipsed eventually by other steps as nn gets huge. For big O analysis, we care most about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as nn gets very large. (If you know what an asymptote is, you might see why &ldquo;big O analysis&rdquo; is sometimes called &ldquo;asymptotic analysis.&rdquo;)

    printf("Fuck");

    int items[] = { 1, 2, 3 };

    void printFirstItem(const int* items)
    {
        printf("%d\n", items[0]);
    }

    int main() {
        printFirstItem(items);
        printFirstItem(items2);
        printf(something);
        printf(items2);
    }

    int main()
    {
      for (int i=0; i<somedata_rows; i++) {
        printf ("%2d %7s ", i, somedata_h(i,"nb"));
        for (int j=1; j<somedata_cols; j++) {
          const char* cell = somedata[i][j];
          printf ("%5s %5g ", cell, 1000*atof(cell));
        }
        printf("\n");
      }
      return 0;
    }

\*\*

\*
